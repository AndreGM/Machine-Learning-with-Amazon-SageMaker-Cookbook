{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innocent-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "import boto3\n",
    "from sagemaker import get_execution_role \n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incredible-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'sagemaker-cookbook-bucket'\n",
    "prefix = 'chapter11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_s3_input_location = f\"s3://{s3_bucket}/{prefix}/input/training.jsonl\"\n",
    "test_s3_input_location = f\"s3://{s3_bucket}/{prefix}/input/test.jsonl\"\n",
    "training_s3_output_location = f\"s3://{s3_bucket}/{prefix}/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "underlying-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train = TrainingInput(training_s3_input_location, content_type=\"json\")\n",
    "test = TrainingInput(test_s3_input_location, content_type=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "monthly-military",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r prediction_length\n",
    "prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "original-robert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r freq\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "communist-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "diagnostic-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve \n",
    "\n",
    "container = retrieve(\"forecasting-deepar\", region_name, \"1\")\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "weird-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    output_path=training_s3_output_location,\n",
    "    sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "funky-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    time_freq=freq,\n",
    "    context_length=str(context_length),\n",
    "    prediction_length=str(prediction_length),\n",
    "    num_cells=40,\n",
    "    num_layers=3,\n",
    "    likelihood=\"gaussian\",\n",
    "    epochs=20,\n",
    "    mini_batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    dropout_rate=0.05,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "asian-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\"train\": train, \"test\": test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "young-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-30 16:04:51 Starting - Starting the training job...\n",
      "2021-03-30 16:04:55 Starting - Launching requested ML instancesProfilerReport-1617120291: InProgress\n",
      ".........\n",
      "2021-03-30 16:06:36 Starting - Preparing the instances for training......\n",
      "2021-03-30 16:07:44 Downloading - Downloading input data...\n",
      "2021-03-30 16:08:17 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '50', 'dropout_rate': '0.05', 'time_freq': 'H', 'likelihood': 'gaussian', 'context_length': '50', 'num_layers': '3', 'epochs': '20', 'learning_rate': '0.001', 'early_stopping_patience': '10', 'mini_batch_size': '32', 'num_cells': '40'}\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.05', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '32', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '3', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '50', 'time_freq': 'H', 'context_length': '50', 'epochs': '20'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] random_seed is None\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/training.jsonl` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/training.jsonl` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Training set statistics:\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Real time series\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] number of time series: 2\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] number of observations: 900\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] mean target length: 450.0\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] min/mean/max target: -225.15890502929688/1737.1891666666668/6317.1669921875\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] mean abs(target): 1744.2852083333332\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Small number of time series. Doing 160 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Test set statistics:\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] Real time series\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:16 INFO 139972648842624] number of time series: 2\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] number of observations: 1000\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] mean target length: 500.0\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] min/mean/max target: -253.2891082763672/1861.5035/7094.15380859375\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] mean abs(target): 1869.13346875\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] #memory_usage::<batchbuffer> = 3.4353256225585938 mb\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] nvidia-smi took: 0.025299072265625 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120497.0308735, \"EndTime\": 1617120497.3084176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 275.2408981323242, \"count\": 1, \"min\": 275.2408981323242, \"max\": 275.2408981323242}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:17 INFO 139972648842624] #memory_usage::<model> = 32 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120497.3085077, \"EndTime\": 1617120497.7289538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 697.9517936706543, \"count\": 1, \"min\": 697.9517936706543, \"max\": 697.9517936706543}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:18 INFO 139972648842624] Epoch[0] Batch[0] avg_epoch_loss=13.625697\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:18 INFO 139972648842624] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=13.625697135925293\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:18 INFO 139972648842624] Epoch[0] Batch[5] avg_epoch_loss=9.998551\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:18 INFO 139972648842624] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=9.998551368713379\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:18 INFO 139972648842624] Epoch[0] Batch [5]#011Speed: 349.20 samples/sec#011loss=9.998551\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] Epoch[0] Batch[10] avg_epoch_loss=9.305996\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=8.47492847442627\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] Epoch[0] Batch [10]#011Speed: 313.68 samples/sec#011loss=8.474928\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] processed a total of 323 examples\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120497.7290492, \"EndTime\": 1617120499.0693383, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 1340.1951789855957, \"count\": 1, \"min\": 1340.1951789855957, \"max\": 1340.1951789855957}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=240.9833491052688 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] #quality_metric: host=algo-1, epoch=0, train loss <loss>=9.305995507673783\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_3eb5836c-668e-409a-8d7e-bd1819961741-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120499.0694432, \"EndTime\": 1617120499.1242342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 54.04496192932129, \"count\": 1, \"min\": 54.04496192932129, \"max\": 54.04496192932129}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] Epoch[1] Batch[0] avg_epoch_loss=8.566374\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=8.566373825073242\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] Epoch[1] Batch[5] avg_epoch_loss=8.871267\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=8.871267000834147\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:19 INFO 139972648842624] Epoch[1] Batch [5]#011Speed: 348.84 samples/sec#011loss=8.871267\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] processed a total of 318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120499.1243243, \"EndTime\": 1617120500.2148647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1090.4643535614014, \"count\": 1, \"min\": 1090.4643535614014, \"max\": 1090.4643535614014}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=291.5808903370654 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] #quality_metric: host=algo-1, epoch=1, train loss <loss>=8.465337896347046\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_402b7368-df12-480c-ae5d-317122426f01-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120500.2149658, \"EndTime\": 1617120500.2551734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.678335189819336, \"count\": 1, \"min\": 39.678335189819336, \"max\": 39.678335189819336}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] Epoch[2] Batch[0] avg_epoch_loss=7.584526\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=7.584526062011719\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] Epoch[2] Batch[5] avg_epoch_loss=7.730430\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=7.730430285135905\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:20 INFO 139972648842624] Epoch[2] Batch [5]#011Speed: 331.65 samples/sec#011loss=7.730430\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] Epoch[2] Batch[10] avg_epoch_loss=7.588910\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=7.4190857887268065\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] Epoch[2] Batch [10]#011Speed: 256.11 samples/sec#011loss=7.419086\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] processed a total of 337 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120500.2552648, \"EndTime\": 1617120501.5953963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1340.0647640228271, \"count\": 1, \"min\": 1340.0647640228271, \"max\": 1340.0647640228271}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=251.45779008685776 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] #quality_metric: host=algo-1, epoch=2, train loss <loss>=7.588910059495405\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_69e1b462-0bfa-4396-864e-f65fc5063830-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120501.595481, \"EndTime\": 1617120501.6476285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 51.64337158203125, \"count\": 1, \"min\": 51.64337158203125, \"max\": 51.64337158203125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] Epoch[3] Batch[0] avg_epoch_loss=7.470232\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:21 INFO 139972648842624] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=7.4702324867248535\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] Epoch[3] Batch[5] avg_epoch_loss=7.259639\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=7.259638945261638\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] Epoch[3] Batch [5]#011Speed: 257.56 samples/sec#011loss=7.259639\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] processed a total of 319 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120501.6477127, \"EndTime\": 1617120502.9486876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1300.903081893921, \"count\": 1, \"min\": 1300.903081893921, \"max\": 1300.903081893921}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=245.18784798379906 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] #quality_metric: host=algo-1, epoch=3, train loss <loss>=7.247583389282227\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:22 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_8f6e7b3b-bea4-4b54-ac65-87e6b3597bd0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120502.948785, \"EndTime\": 1617120503.002335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 52.98209190368652, \"count\": 1, \"min\": 52.98209190368652, \"max\": 52.98209190368652}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] Epoch[4] Batch[0] avg_epoch_loss=7.032602\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=7.032602310180664\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] Epoch[4] Batch[5] avg_epoch_loss=7.063562\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=7.063561995824178\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] Epoch[4] Batch [5]#011Speed: 353.08 samples/sec#011loss=7.063562\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] processed a total of 288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120503.002412, \"EndTime\": 1617120503.9970038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 994.5251941680908, \"count\": 1, \"min\": 994.5251941680908, \"max\": 994.5251941680908}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=289.542457741685 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] #quality_metric: host=algo-1, epoch=4, train loss <loss>=7.023451275295681\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:23 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:24 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_44a5ef30-920d-48ef-81d9-773b08817eaa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120503.9971082, \"EndTime\": 1617120504.0476365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 49.42178726196289, \"count\": 1, \"min\": 49.42178726196289, \"max\": 49.42178726196289}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:24 INFO 139972648842624] Epoch[5] Batch[0] avg_epoch_loss=6.928497\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:24 INFO 139972648842624] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=6.928496837615967\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:24 INFO 139972648842624] Epoch[5] Batch[5] avg_epoch_loss=6.865604\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:24 INFO 139972648842624] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=6.865604241689046\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:24 INFO 139972648842624] Epoch[5] Batch [5]#011Speed: 333.31 samples/sec#011loss=6.865604\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] processed a total of 301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120504.0477242, \"EndTime\": 1617120505.172974, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1125.1683235168457, \"count\": 1, \"min\": 1125.1683235168457, \"max\": 1125.1683235168457}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=267.4811826681221 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] #quality_metric: host=algo-1, epoch=5, train loss <loss>=6.83504056930542\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_7beec3df-d07c-4472-a6d0-644d9cacd2ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120505.173074, \"EndTime\": 1617120505.2044792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 30.75242042541504, \"count\": 1, \"min\": 30.75242042541504, \"max\": 30.75242042541504}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] Epoch[6] Batch[0] avg_epoch_loss=6.742172\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.7421722412109375\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] Epoch[6] Batch[5] avg_epoch_loss=6.814130\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=6.814130067825317\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:25 INFO 139972648842624] Epoch[6] Batch [5]#011Speed: 347.04 samples/sec#011loss=6.814130\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] Epoch[6] Batch[10] avg_epoch_loss=6.803095\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=6.789853954315186\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] Epoch[6] Batch [10]#011Speed: 320.82 samples/sec#011loss=6.789854\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] processed a total of 330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120505.2045786, \"EndTime\": 1617120506.4017794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1197.1395015716553, \"count\": 1, \"min\": 1197.1395015716553, \"max\": 1197.1395015716553}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=275.62613768041336 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] #quality_metric: host=algo-1, epoch=6, train loss <loss>=6.803095470775258\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_348bbefb-407a-4ba8-9c2c-09a8d5e60067-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120506.4018774, \"EndTime\": 1617120506.4386864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 36.24105453491211, \"count\": 1, \"min\": 36.24105453491211, \"max\": 36.24105453491211}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] Epoch[7] Batch[0] avg_epoch_loss=6.653329\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:26 INFO 139972648842624] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=6.653328895568848\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] Epoch[7] Batch[5] avg_epoch_loss=6.692307\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=6.692306677500407\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] Epoch[7] Batch [5]#011Speed: 313.86 samples/sec#011loss=6.692307\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] processed a total of 306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120506.4387662, \"EndTime\": 1617120507.5519023, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1113.070011138916, \"count\": 1, \"min\": 1113.070011138916, \"max\": 1113.070011138916}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=274.8818780061862 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] #quality_metric: host=algo-1, epoch=7, train loss <loss>=6.673714208602905\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_5c4f3db0-0f67-4a1e-9afc-f32a932418c3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120507.551997, \"EndTime\": 1617120507.5938954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 41.33439064025879, \"count\": 1, \"min\": 41.33439064025879, \"max\": 41.33439064025879}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] Epoch[8] Batch[0] avg_epoch_loss=6.567502\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:27 INFO 139972648842624] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=6.567502498626709\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] Epoch[8] Batch[5] avg_epoch_loss=6.621550\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=6.621549606323242\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] Epoch[8] Batch [5]#011Speed: 341.11 samples/sec#011loss=6.621550\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] processed a total of 317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120507.5939994, \"EndTime\": 1617120508.708811, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1114.74609375, \"count\": 1, \"min\": 1114.74609375, \"max\": 1114.74609375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=284.33715395181343 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] #quality_metric: host=algo-1, epoch=8, train loss <loss>=6.614810848236084\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_f6b4281f-8338-4282-9bc3-ee1638b104c2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120508.7089016, \"EndTime\": 1617120508.74141, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 31.906604766845703, \"count\": 1, \"min\": 31.906604766845703, \"max\": 31.906604766845703}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] Epoch[9] Batch[0] avg_epoch_loss=6.480358\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:28 INFO 139972648842624] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=6.480358123779297\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] Epoch[9] Batch[5] avg_epoch_loss=6.523868\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=6.523868481318156\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] Epoch[9] Batch [5]#011Speed: 324.03 samples/sec#011loss=6.523868\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] Epoch[9] Batch[10] avg_epoch_loss=6.491663\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=6.453016757965088\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] Epoch[9] Batch [10]#011Speed: 325.32 samples/sec#011loss=6.453017\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] processed a total of 327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120508.7414796, \"EndTime\": 1617120509.9748204, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1233.2823276519775, \"count\": 1, \"min\": 1233.2823276519775, \"max\": 1233.2823276519775}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=265.1033068681822 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] #quality_metric: host=algo-1, epoch=9, train loss <loss>=6.491663152521307\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:29 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:30 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_bdae8bd6-1409-4099-9200-93974d5e6187-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120509.9749794, \"EndTime\": 1617120510.026258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 50.55856704711914, \"count\": 1, \"min\": 50.55856704711914, \"max\": 50.55856704711914}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:30 INFO 139972648842624] Epoch[10] Batch[0] avg_epoch_loss=6.586048\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:30 INFO 139972648842624] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=6.586047649383545\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[03/30/2021 16:08:30 INFO 139972648842624] Epoch[10] Batch[5] avg_epoch_loss=6.499100\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:30 INFO 139972648842624] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=6.499099572499593\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:30 INFO 139972648842624] Epoch[10] Batch [5]#011Speed: 354.07 samples/sec#011loss=6.499100\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] processed a total of 302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120510.0263429, \"EndTime\": 1617120511.1115274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1085.1130485534668, \"count\": 1, \"min\": 1085.1130485534668, \"max\": 1085.1130485534668}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=278.27789831479583 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] #quality_metric: host=algo-1, epoch=10, train loss <loss>=6.509853029251099\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] Epoch[11] Batch[0] avg_epoch_loss=6.507777\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=6.507777214050293\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] Epoch[11] Batch[5] avg_epoch_loss=6.398640\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=6.398640235265096\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:31 INFO 139972648842624] Epoch[11] Batch [5]#011Speed: 351.27 samples/sec#011loss=6.398640\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] Epoch[11] Batch[10] avg_epoch_loss=6.442159\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=6.49438247680664\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] Epoch[11] Batch [10]#011Speed: 310.28 samples/sec#011loss=6.494382\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] processed a total of 341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120511.1116238, \"EndTime\": 1617120512.325862, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1213.693380355835, \"count\": 1, \"min\": 1213.693380355835, \"max\": 1213.693380355835}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=280.92758225619633 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] #quality_metric: host=algo-1, epoch=11, train loss <loss>=6.442159435965798\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_4575d9d8-1a58-4ed4-ba75-7f2adc88b425-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120512.3259706, \"EndTime\": 1617120512.3787928, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 52.28996276855469, \"count\": 1, \"min\": 52.28996276855469, \"max\": 52.28996276855469}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] Epoch[12] Batch[0] avg_epoch_loss=6.393719\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:32 INFO 139972648842624] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=6.393718719482422\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] Epoch[12] Batch[5] avg_epoch_loss=6.373804\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=6.373803933461507\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] Epoch[12] Batch [5]#011Speed: 350.58 samples/sec#011loss=6.373804\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] Epoch[12] Batch[10] avg_epoch_loss=6.369335\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=6.363972759246826\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] Epoch[12] Batch [10]#011Speed: 318.86 samples/sec#011loss=6.363973\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] processed a total of 343 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120512.3788714, \"EndTime\": 1617120513.5868561, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1207.9157829284668, \"count\": 1, \"min\": 1207.9157829284668, \"max\": 1207.9157829284668}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=283.9298191322992 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] #quality_metric: host=algo-1, epoch=12, train loss <loss>=6.36933521790938\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_91157aa9-5b9f-44aa-bb0c-f65d3ad75cbe-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120513.5869443, \"EndTime\": 1617120513.6396627, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 52.190542221069336, \"count\": 1, \"min\": 52.190542221069336, \"max\": 52.190542221069336}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] Epoch[13] Batch[0] avg_epoch_loss=6.409207\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:33 INFO 139972648842624] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=6.409207344055176\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] Epoch[13] Batch[5] avg_epoch_loss=6.362521\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=6.362520774205525\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] Epoch[13] Batch [5]#011Speed: 306.20 samples/sec#011loss=6.362521\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] Epoch[13] Batch[10] avg_epoch_loss=6.410720\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=6.468558597564697\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] Epoch[13] Batch [10]#011Speed: 327.42 samples/sec#011loss=6.468559\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] processed a total of 326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120513.6397517, \"EndTime\": 1617120514.898069, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1258.2480907440186, \"count\": 1, \"min\": 1258.2480907440186, \"max\": 1258.2480907440186}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=259.06413667272324 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] #quality_metric: host=algo-1, epoch=13, train loss <loss>=6.410719784823331\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:34 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:35 INFO 139972648842624] Epoch[14] Batch[0] avg_epoch_loss=6.311059\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:35 INFO 139972648842624] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=6.31105899810791\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:35 INFO 139972648842624] Epoch[14] Batch[5] avg_epoch_loss=6.384324\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:35 INFO 139972648842624] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=6.384324391682942\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:35 INFO 139972648842624] Epoch[14] Batch [5]#011Speed: 352.14 samples/sec#011loss=6.384324\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] Epoch[14] Batch[10] avg_epoch_loss=6.375706\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=6.365364646911621\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] Epoch[14] Batch [10]#011Speed: 348.92 samples/sec#011loss=6.365365\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] processed a total of 335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120514.8981588, \"EndTime\": 1617120516.051954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1153.273105621338, \"count\": 1, \"min\": 1153.273105621338, \"max\": 1153.273105621338}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=290.4444522534329 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] #quality_metric: host=algo-1, epoch=14, train loss <loss>=6.375706325877797\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] Epoch[15] Batch[0] avg_epoch_loss=6.481248\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=6.481247901916504\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] Epoch[15] Batch[5] avg_epoch_loss=6.380294\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=6.380293528238933\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:36 INFO 139972648842624] Epoch[15] Batch [5]#011Speed: 352.88 samples/sec#011loss=6.380294\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] processed a total of 303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120516.0520444, \"EndTime\": 1617120517.151408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1098.8595485687256, \"count\": 1, \"min\": 1098.8595485687256, \"max\": 1098.8595485687256}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=275.710754890988 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] #quality_metric: host=algo-1, epoch=15, train loss <loss>=6.377045822143555\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] Epoch[16] Batch[0] avg_epoch_loss=6.416497\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=6.416497230529785\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] Epoch[16] Batch[5] avg_epoch_loss=6.382818\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=6.382818380991618\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:37 INFO 139972648842624] Epoch[16] Batch [5]#011Speed: 309.16 samples/sec#011loss=6.382818\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] processed a total of 276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120517.1514869, \"EndTime\": 1617120518.196481, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1044.4655418395996, \"count\": 1, \"min\": 1044.4655418395996, \"max\": 1044.4655418395996}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=264.21500045875894 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] #quality_metric: host=algo-1, epoch=16, train loss <loss>=6.3458542293972435\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/state_7916bc1d-5345-45d7-99fa-5b40429b6df1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120518.1965754, \"EndTime\": 1617120518.2396746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 42.50979423522949, \"count\": 1, \"min\": 42.50979423522949, \"max\": 42.50979423522949}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] Epoch[17] Batch[0] avg_epoch_loss=6.241517\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=6.241516590118408\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] Epoch[17] Batch[5] avg_epoch_loss=6.335193\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=6.335192600886027\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:38 INFO 139972648842624] Epoch[17] Batch [5]#011Speed: 350.36 samples/sec#011loss=6.335193\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] processed a total of 304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120518.239749, \"EndTime\": 1617120519.3345468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1094.7391986846924, \"count\": 1, \"min\": 1094.7391986846924, \"max\": 1094.7391986846924}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=277.64829401354893 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] #quality_metric: host=algo-1, epoch=17, train loss <loss>=6.350082015991211\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] Epoch[18] Batch[0] avg_epoch_loss=6.844417\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:39 INFO 139972648842624] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=6.844416618347168\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] Epoch[18] Batch[5] avg_epoch_loss=6.612155\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=6.612155437469482\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] Epoch[18] Batch [5]#011Speed: 353.95 samples/sec#011loss=6.612155\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-03-30 16:08:52 Uploading - Uploading generated training model\n",
      "2021-03-30 16:08:52 Completed - Training job completed\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] processed a total of 309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120519.334677, \"EndTime\": 1617120520.4030344, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1067.5427913665771, \"count\": 1, \"min\": 1067.5427913665771, \"max\": 1067.5427913665771}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=289.4139472233307 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] #quality_metric: host=algo-1, epoch=18, train loss <loss>=6.56499719619751\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] Epoch[19] Batch[0] avg_epoch_loss=6.576227\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:40 INFO 139972648842624] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=6.576226711273193\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] Epoch[19] Batch[5] avg_epoch_loss=6.442052\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=6.442051966985066\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] Epoch[19] Batch [5]#011Speed: 352.71 samples/sec#011loss=6.442052\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] Epoch[19] Batch[10] avg_epoch_loss=6.418566\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=6.390383338928222\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] Epoch[19] Batch [10]#011Speed: 345.66 samples/sec#011loss=6.390383\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] processed a total of 324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120520.403127, \"EndTime\": 1617120521.5654786, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1161.8123054504395, \"count\": 1, \"min\": 1161.8123054504395, \"max\": 1161.8123054504395}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] #throughput_metric: host=algo-1, train throughput=278.84672519566095 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] #quality_metric: host=algo-1, epoch=19, train loss <loss>=6.4185662269592285\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] Final loss: 6.3458542293972435 (occurred at epoch 16)\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] #quality_metric: host=algo-1, train final_loss <loss>=6.3458542293972435\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 WARNING 139972648842624] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:41 INFO 139972648842624] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120521.5655577, \"EndTime\": 1617120521.962622, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 395.9529399871826, \"count\": 1, \"min\": 395.9529399871826, \"max\": 395.9529399871826}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:42 INFO 139972648842624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120521.9627047, \"EndTime\": 1617120522.1247215, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 558.0952167510986, \"count\": 1, \"min\": 558.0952167510986, \"max\": 558.0952167510986}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:42 INFO 139972648842624] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:42 INFO 139972648842624] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120522.1248007, \"EndTime\": 1617120522.1653163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 40.464162826538086, \"count\": 1, \"min\": 40.464162826538086, \"max\": 40.464162826538086}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:42 INFO 139972648842624] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:42 INFO 139972648842624] #memory_usage::<batchbuffer> = 3.4353256225585938 mb\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:42 INFO 139972648842624] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120522.1653895, \"EndTime\": 1617120522.167299, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04291534423828125, \"count\": 1, \"min\": 0.04291534423828125, \"max\": 0.04291534423828125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120522.1673715, \"EndTime\": 1617120523.648196, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 1480.9355735778809, \"count\": 1, \"min\": 1480.9355735778809, \"max\": 1480.9355735778809}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, RMSE): 453.0285035182665\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, mean_absolute_QuantileLoss): 27533.900039291377\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, mean_wQuantileLoss): 0.09200150542760024\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.1]): 0.03769916591921211\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.2]): 0.06689544435055812\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.3]): 0.08808146859956725\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.4]): 0.10374580313001243\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.5]): 0.11406234763217583\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.6]): 0.11966243751118907\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.7]): 0.1158978879714173\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.8]): 0.10326848612781697\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #test_score (algo-1, wQuantileLoss[0.9]): 0.07870050760645304\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #quality_metric: host=algo-1, test RMSE <loss>=453.0285035182665\u001b[0m\n",
      "\u001b[34m[03/30/2021 16:08:43 INFO 139972648842624] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.09200150542760024\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1617120523.6483269, \"EndTime\": 1617120523.6931589, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.673025131225586, \"count\": 1, \"min\": 7.673025131225586, \"max\": 7.673025131225586}, \"totaltime\": {\"sum\": 26987.788677215576, \"count\": 1, \"min\": 26987.788677215576, \"max\": 26987.788677215576}}}\n",
      "\u001b[0m\n",
      "Training seconds: 68\n",
      "Billable seconds: 68\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rotary-helmet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-cookbook-bucket/chapter11/output/forecasting-deepar-2021-03-30-16-04-51-022/output/model.tar.gz'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "continuous-klein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "governmental-indonesia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forecasting-deepar-2021-03-30-16-09-46-185'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dynamic-garden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-sociology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
