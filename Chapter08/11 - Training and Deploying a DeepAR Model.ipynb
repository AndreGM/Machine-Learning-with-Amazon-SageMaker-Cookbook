{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "import boto3\n",
    "from sagemaker import get_execution_role \n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r s3_bucket\n",
    "%store -r prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_s3_input_location = f\"s3://{s3_bucket}/{prefix}/input/training.jsonl\"\n",
    "test_s3_input_location = f\"s3://{s3_bucket}/{prefix}/input/test.jsonl\"\n",
    "training_s3_output_location = f\"s3://{s3_bucket}/{prefix}/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "train = TrainingInput(\n",
    "    training_s3_input_location, \n",
    "    content_type=\"json\"\n",
    ")\n",
    "test = TrainingInput(\n",
    "    test_s3_input_location, \n",
    "    content_type=\"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r prediction_length\n",
    "prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r freq\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve \n",
    "container = retrieve(\n",
    "    \"forecasting-deepar\", \n",
    "    region_name, \n",
    "    \"1\"\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    output_path=training_s3_output_location,\n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    time_freq=freq,\n",
    "    context_length=str(context_length),\n",
    "    prediction_length=str(prediction_length),\n",
    "    num_cells=40,\n",
    "    num_layers=3,\n",
    "    likelihood=\"gaussian\",\n",
    "    epochs=20,\n",
    "    mini_batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    dropout_rate=0.05,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\"train\": train, \"test\": test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01 11:05:17 Starting - Starting the training job...\n",
      "2021-06-01 11:05:18 Starting - Launching requested ML instancesProfilerReport-1622545517: InProgress\n",
      "......\n",
      "2021-06-01 11:06:44 Starting - Preparing the instances for training.........\n",
      "2021-06-01 11:08:14 Downloading - Downloading input data\n",
      "2021-06-01 11:08:14 Training - Downloading the training image...\n",
      "2021-06-01 11:08:35 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '50', 'dropout_rate': '0.05', 'time_freq': 'H', 'likelihood': 'gaussian', 'context_length': '50', 'num_layers': '3', 'epochs': '20', 'learning_rate': '0.001', 'early_stopping_patience': '10', 'mini_batch_size': '32', 'num_cells': '40'}\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.05', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '32', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '3', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '50', 'time_freq': 'H', 'context_length': '50', 'epochs': '20'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] random_seed is None\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/training.jsonl` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/training.jsonl` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] number of time series: 1\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] number of observations: 450\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] mean target length: 450.0\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] min/mean/max target: -216.7050323486328/611.7573611111111/2012.281494140625\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] mean abs(target): 628.5100694444444\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Small number of time series. Doing 320 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] number of time series: 1\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] number of observations: 500\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] mean target length: 500.0\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] min/mean/max target: -216.7050323486328/613.425625/2092.252685546875\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] mean abs(target): 628.866125\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] #memory_usage::<batchbuffer> = 3.4353256225585938 mb\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] nvidia-smi took: 0.02534794807434082 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545719.5530539, \"EndTime\": 1622545719.8465643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 291.04113578796387, \"count\": 1, \"min\": 291.04113578796387, \"max\": 291.04113578796387}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:39 INFO 139987855828352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:40 INFO 139987855828352] #memory_usage::<model> = 32 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545719.8466432, \"EndTime\": 1622545720.2368925, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 683.7303638458252, \"count\": 1, \"min\": 683.7303638458252, \"max\": 683.7303638458252}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:40 INFO 139987855828352] Epoch[0] Batch[0] avg_epoch_loss=9.861291\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:40 INFO 139987855828352] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.86129093170166\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] Epoch[0] Batch[5] avg_epoch_loss=8.662215\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=8.66221515337626\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] Epoch[0] Batch [5]#011Speed: 360.05 samples/sec#011loss=8.662215\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] Epoch[0] Batch[10] avg_epoch_loss=8.254310\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=7.764824771881104\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] Epoch[0] Batch [10]#011Speed: 262.10 samples/sec#011loss=7.764825\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] processed a total of 351 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545720.2369604, \"EndTime\": 1622545721.6986253, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 1461.578607559204, \"count\": 1, \"min\": 1461.578607559204, \"max\": 1461.578607559204}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=240.1271691479959 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.254310434514826\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:41 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_27137cea-fde9-43cd-9e25-6fa8714fb83f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545721.6987271, \"EndTime\": 1622545721.7408478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 41.533708572387695, \"count\": 1, \"min\": 41.533708572387695, \"max\": 41.533708572387695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:42 INFO 139987855828352] Epoch[1] Batch[0] avg_epoch_loss=7.698203\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:42 INFO 139987855828352] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=7.698202610015869\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:42 INFO 139987855828352] Epoch[1] Batch[5] avg_epoch_loss=7.559650\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:42 INFO 139987855828352] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=7.559650023778279\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:42 INFO 139987855828352] Epoch[1] Batch [5]#011Speed: 317.43 samples/sec#011loss=7.559650\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] Epoch[1] Batch[10] avg_epoch_loss=7.481126\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=7.3868976593017575\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] Epoch[1] Batch [10]#011Speed: 353.61 samples/sec#011loss=7.386898\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] processed a total of 323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545721.7409396, \"EndTime\": 1622545723.01716, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1276.1547565460205, \"count\": 1, \"min\": 1276.1547565460205, \"max\": 1276.1547565460205}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=253.07941804968073 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] #quality_metric: host=algo-1, epoch=1, train loss <loss>=7.481126221743497\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_457f841e-c4f9-477c-84ae-4aa844d0916a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545723.0172472, \"EndTime\": 1622545723.0691602, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 51.4063835144043, \"count\": 1, \"min\": 51.4063835144043, \"max\": 51.4063835144043}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] Epoch[2] Batch[0] avg_epoch_loss=7.128631\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=7.128631114959717\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] Epoch[2] Batch[5] avg_epoch_loss=7.124743\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=7.124743064244588\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:43 INFO 139987855828352] Epoch[2] Batch [5]#011Speed: 317.19 samples/sec#011loss=7.124743\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] processed a total of 314 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545723.069232, \"EndTime\": 1622545724.1994393, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1130.1512718200684, \"count\": 1, \"min\": 1130.1512718200684, \"max\": 1130.1512718200684}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=277.81019463841534 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] #quality_metric: host=algo-1, epoch=2, train loss <loss>=7.085926914215088\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_1ce4d73c-8e3a-462c-a489-6dd1895b6f84-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545724.199523, \"EndTime\": 1622545724.231989, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 31.908512115478516, \"count\": 1, \"min\": 31.908512115478516, \"max\": 31.908512115478516}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] Epoch[3] Batch[0] avg_epoch_loss=6.963052\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=6.963051795959473\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] Epoch[3] Batch[5] avg_epoch_loss=6.750784\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=6.750784397125244\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:44 INFO 139987855828352] Epoch[3] Batch [5]#011Speed: 349.99 samples/sec#011loss=6.750784\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] processed a total of 317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545724.2320597, \"EndTime\": 1622545725.2935164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1061.3934993743896, \"count\": 1, \"min\": 1061.3934993743896, \"max\": 1061.3934993743896}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=298.62949414473934 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] #quality_metric: host=algo-1, epoch=3, train loss <loss>=6.7134730339050295\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_c1403149-2b7d-42bd-b551-994b23f85e57-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545725.2935963, \"EndTime\": 1622545725.3263936, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 32.241106033325195, \"count\": 1, \"min\": 32.241106033325195, \"max\": 32.241106033325195}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] Epoch[4] Batch[0] avg_epoch_loss=6.513759\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:45 INFO 139987855828352] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=6.513758659362793\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] Epoch[4] Batch[5] avg_epoch_loss=6.551446\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=6.551445881525676\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] Epoch[4] Batch [5]#011Speed: 354.52 samples/sec#011loss=6.551446\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] Epoch[4] Batch[10] avg_epoch_loss=6.566663\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=6.58492259979248\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] Epoch[4] Batch [10]#011Speed: 328.12 samples/sec#011loss=6.584923\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] processed a total of 334 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545725.3264642, \"EndTime\": 1622545726.4947262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1168.1983470916748, \"count\": 1, \"min\": 1168.1983470916748, \"max\": 1168.1983470916748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=285.8828121253381 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] #quality_metric: host=algo-1, epoch=4, train loss <loss>=6.56666257164695\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_71efb267-da57-4106-a23d-106545e7ee4a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545726.4948018, \"EndTime\": 1622545726.5278127, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 32.45115280151367, \"count\": 1, \"min\": 32.45115280151367, \"max\": 32.45115280151367}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] Epoch[5] Batch[0] avg_epoch_loss=6.436613\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:46 INFO 139987855828352] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=6.436613082885742\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] Epoch[5] Batch[5] avg_epoch_loss=6.373115\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=6.3731147448221845\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] Epoch[5] Batch [5]#011Speed: 354.71 samples/sec#011loss=6.373115\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] Epoch[5] Batch[10] avg_epoch_loss=6.353958\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=6.330970573425293\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] Epoch[5] Batch [10]#011Speed: 341.48 samples/sec#011loss=6.330971\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] processed a total of 332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545726.527898, \"EndTime\": 1622545727.6816957, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1153.7244319915771, \"count\": 1, \"min\": 1153.7244319915771, \"max\": 1153.7244319915771}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=287.73330589294693 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] #quality_metric: host=algo-1, epoch=5, train loss <loss>=6.353958303278143\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_ff948819-a8e9-4b18-9b70-5c102ce9742a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545727.681776, \"EndTime\": 1622545727.7138128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 31.453847885131836, \"count\": 1, \"min\": 31.453847885131836, \"max\": 31.453847885131836}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] Epoch[6] Batch[0] avg_epoch_loss=6.385988\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:47 INFO 139987855828352] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.385987758636475\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] Epoch[6] Batch[5] avg_epoch_loss=6.329630\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=6.329630374908447\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] Epoch[6] Batch [5]#011Speed: 333.87 samples/sec#011loss=6.329630\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] processed a total of 317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545727.713888, \"EndTime\": 1622545728.7979345, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1083.9884281158447, \"count\": 1, \"min\": 1083.9884281158447, \"max\": 1083.9884281158447}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=292.40233817300543 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] #quality_metric: host=algo-1, epoch=6, train loss <loss>=6.310426473617554\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:48 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_46a54089-8378-48ac-905a-2fb38c660e8a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545728.79803, \"EndTime\": 1622545728.8498664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 51.26786231994629, \"count\": 1, \"min\": 51.26786231994629, \"max\": 51.26786231994629}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] Epoch[7] Batch[0] avg_epoch_loss=6.345491\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=6.3454909324646\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] Epoch[7] Batch[5] avg_epoch_loss=6.252596\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=6.252595901489258\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] Epoch[7] Batch [5]#011Speed: 349.02 samples/sec#011loss=6.252596\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] processed a total of 310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545728.8499393, \"EndTime\": 1622545729.9476416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1097.6393222808838, \"count\": 1, \"min\": 1097.6393222808838, \"max\": 1097.6393222808838}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=282.39269490490295 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] #quality_metric: host=algo-1, epoch=7, train loss <loss>=6.256040334701538\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:49 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_ca623117-6bb8-4249-a863-c19e6a91b56d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545729.9477239, \"EndTime\": 1622545729.9797022, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 31.38279914855957, \"count\": 1, \"min\": 31.38279914855957, \"max\": 31.38279914855957}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:50 INFO 139987855828352] Epoch[8] Batch[0] avg_epoch_loss=6.273207\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:50 INFO 139987855828352] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=6.273207187652588\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:50 INFO 139987855828352] Epoch[8] Batch[5] avg_epoch_loss=6.240268\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:50 INFO 139987855828352] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=6.2402684688568115\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:50 INFO 139987855828352] Epoch[8] Batch [5]#011Speed: 346.20 samples/sec#011loss=6.240268\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] processed a total of 307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545729.9797711, \"EndTime\": 1622545731.0801375, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1100.3057956695557, \"count\": 1, \"min\": 1100.3057956695557, \"max\": 1100.3057956695557}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=278.9799758166328 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] #quality_metric: host=algo-1, epoch=8, train loss <loss>=6.20835280418396\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_7bd77aee-8296-4054-add0-75024c681189-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545731.080231, \"EndTime\": 1622545731.1216967, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.93003273010254, \"count\": 1, \"min\": 40.93003273010254, \"max\": 40.93003273010254}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] Epoch[9] Batch[0] avg_epoch_loss=6.259841\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=6.259840965270996\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] Epoch[9] Batch[5] avg_epoch_loss=6.193102\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=6.193101803461711\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:51 INFO 139987855828352] Epoch[9] Batch [5]#011Speed: 286.33 samples/sec#011loss=6.193102\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] Epoch[9] Batch[10] avg_epoch_loss=6.159340\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=6.118824768066406\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] Epoch[9] Batch [10]#011Speed: 341.94 samples/sec#011loss=6.118825\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] processed a total of 339 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545731.1217747, \"EndTime\": 1622545732.4023514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1280.510663986206, \"count\": 1, \"min\": 1280.510663986206, \"max\": 1280.510663986206}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=264.7123540457246 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] #quality_metric: host=algo-1, epoch=9, train loss <loss>=6.159339514645663\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_c0a73cbf-daba-4e17-bf4c-ef7b42862a3d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545732.402438, \"EndTime\": 1622545732.4542758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 51.33199691772461, \"count\": 1, \"min\": 51.33199691772461, \"max\": 51.33199691772461}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] Epoch[10] Batch[0] avg_epoch_loss=6.205497\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:52 INFO 139987855828352] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=6.2054972648620605\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] Epoch[10] Batch[5] avg_epoch_loss=6.158334\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=6.158334255218506\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] Epoch[10] Batch [5]#011Speed: 355.64 samples/sec#011loss=6.158334\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] Epoch[10] Batch[10] avg_epoch_loss=6.145585\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=6.130285167694092\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] Epoch[10] Batch [10]#011Speed: 338.03 samples/sec#011loss=6.130285\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] processed a total of 336 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545732.4543521, \"EndTime\": 1622545733.6398988, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1185.487985610962, \"count\": 1, \"min\": 1185.487985610962, \"max\": 1185.487985610962}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=283.4073536615874 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] #quality_metric: host=algo-1, epoch=10, train loss <loss>=6.145584669980136\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_36fdacab-037b-4f7d-8d2c-03b891d1fc05-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545733.6399536, \"EndTime\": 1622545733.6726034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 32.32264518737793, \"count\": 1, \"min\": 32.32264518737793, \"max\": 32.32264518737793}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] Epoch[11] Batch[0] avg_epoch_loss=6.087798\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:53 INFO 139987855828352] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=6.08779764175415\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] Epoch[11] Batch[5] avg_epoch_loss=6.095895\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=6.095895449320476\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] Epoch[11] Batch [5]#011Speed: 357.20 samples/sec#011loss=6.095895\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] Epoch[11] Batch[10] avg_epoch_loss=6.101880\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=6.1090624809265135\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] Epoch[11] Batch [10]#011Speed: 338.08 samples/sec#011loss=6.109062\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] processed a total of 329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545733.672711, \"EndTime\": 1622545734.8327868, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1159.998893737793, \"count\": 1, \"min\": 1159.998893737793, \"max\": 1159.998893737793}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=283.58919368597964 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] #quality_metric: host=algo-1, epoch=11, train loss <loss>=6.101880463686856\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:54 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_6f78f0db-18ac-455d-b5eb-48f193fe725c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545734.8328738, \"EndTime\": 1622545734.8648176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 31.39781951904297, \"count\": 1, \"min\": 31.39781951904297, \"max\": 31.39781951904297}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:55 INFO 139987855828352] Epoch[12] Batch[0] avg_epoch_loss=6.033226\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:55 INFO 139987855828352] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=6.0332255363464355\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:55 INFO 139987855828352] Epoch[12] Batch[5] avg_epoch_loss=6.071375\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:55 INFO 139987855828352] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=6.071375449498494\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:55 INFO 139987855828352] Epoch[12] Batch [5]#011Speed: 340.37 samples/sec#011loss=6.071375\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] Epoch[12] Batch[10] avg_epoch_loss=6.104566\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=6.144394493103027\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] Epoch[12] Batch [10]#011Speed: 351.24 samples/sec#011loss=6.144394\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] processed a total of 324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545734.8648915, \"EndTime\": 1622545736.017775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1152.815818786621, \"count\": 1, \"min\": 1152.815818786621, \"max\": 1152.815818786621}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=281.0201634065995 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] #quality_metric: host=algo-1, epoch=12, train loss <loss>=6.104565923864191\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] Epoch[13] Batch[0] avg_epoch_loss=6.033755\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=6.033754825592041\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] Epoch[13] Batch[5] avg_epoch_loss=6.053319\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=6.0533192952473955\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:56 INFO 139987855828352] Epoch[13] Batch [5]#011Speed: 312.50 samples/sec#011loss=6.053319\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] Epoch[13] Batch[10] avg_epoch_loss=5.931203\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=5.784662437438965\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] Epoch[13] Batch [10]#011Speed: 343.24 samples/sec#011loss=5.784662\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] processed a total of 321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545736.017862, \"EndTime\": 1622545737.248619, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1230.2217483520508, \"count\": 1, \"min\": 1230.2217483520508, \"max\": 1230.2217483520508}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=260.90055688639694 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] #quality_metric: host=algo-1, epoch=13, train loss <loss>=5.931202541698109\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_a5b2e4a3-fe22-4bcd-921c-6f0c55d6491f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545737.2487123, \"EndTime\": 1622545737.2805972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 31.32152557373047, \"count\": 1, \"min\": 31.32152557373047, \"max\": 31.32152557373047}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] Epoch[14] Batch[0] avg_epoch_loss=6.039817\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=6.0398173332214355\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] Epoch[14] Batch[5] avg_epoch_loss=5.982143\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=5.982143402099609\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:57 INFO 139987855828352] Epoch[14] Batch [5]#011Speed: 354.54 samples/sec#011loss=5.982143\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] Epoch[14] Batch[10] avg_epoch_loss=5.992226\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=6.004324245452881\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] Epoch[14] Batch [10]#011Speed: 345.99 samples/sec#011loss=6.004324\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] processed a total of 323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545737.2806687, \"EndTime\": 1622545738.4378157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1157.0823192596436, \"count\": 1, \"min\": 1157.0823192596436, \"max\": 1157.0823192596436}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=279.1203848134105 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] #quality_metric: host=algo-1, epoch=14, train loss <loss>=5.992225603623823\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] Epoch[15] Batch[0] avg_epoch_loss=6.078166\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:58 INFO 139987855828352] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=6.078166484832764\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] Epoch[15] Batch[5] avg_epoch_loss=6.047622\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=6.047621965408325\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] Epoch[15] Batch [5]#011Speed: 359.76 samples/sec#011loss=6.047622\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] Epoch[15] Batch[10] avg_epoch_loss=6.016156\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=5.9783961296081545\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] Epoch[15] Batch [10]#011Speed: 344.35 samples/sec#011loss=5.978396\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] processed a total of 325 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545738.4379017, \"EndTime\": 1622545739.6137993, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1175.3809452056885, \"count\": 1, \"min\": 1175.3809452056885, \"max\": 1175.3809452056885}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=276.4689179826746 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] #quality_metric: host=algo-1, epoch=15, train loss <loss>=6.016155676408247\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] Epoch[16] Batch[0] avg_epoch_loss=6.039296\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:08:59 INFO 139987855828352] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=6.0392961502075195\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] Epoch[16] Batch[5] avg_epoch_loss=6.021959\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=6.02195946375529\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] Epoch[16] Batch [5]#011Speed: 345.43 samples/sec#011loss=6.021959\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] Epoch[16] Batch[10] avg_epoch_loss=5.931105\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=5.822079753875732\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] Epoch[16] Batch [10]#011Speed: 344.53 samples/sec#011loss=5.822080\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] processed a total of 324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545739.6139174, \"EndTime\": 1622545740.8288224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1214.3797874450684, \"count\": 1, \"min\": 1214.3797874450684, \"max\": 1214.3797874450684}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=266.77557180517704 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] #quality_metric: host=algo-1, epoch=16, train loss <loss>=5.931105050173673\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:00 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/state_09945a18-505e-4c09-92e4-8074438406ab-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545740.8289082, \"EndTime\": 1622545740.8625147, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 33.09965133666992, \"count\": 1, \"min\": 33.09965133666992, \"max\": 33.09965133666992}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:01 INFO 139987855828352] Epoch[17] Batch[0] avg_epoch_loss=6.057468\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:01 INFO 139987855828352] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=6.057468414306641\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:01 INFO 139987855828352] Epoch[17] Batch[5] avg_epoch_loss=5.931457\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:01 INFO 139987855828352] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=5.931456804275513\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:01 INFO 139987855828352] Epoch[17] Batch [5]#011Speed: 294.05 samples/sec#011loss=5.931457\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] processed a total of 305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545740.862593, \"EndTime\": 1622545742.0764163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1213.7548923492432, \"count\": 1, \"min\": 1213.7548923492432, \"max\": 1213.7548923492432}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=251.25956932465908 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] #quality_metric: host=algo-1, epoch=17, train loss <loss>=5.960087680816651\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] Epoch[18] Batch[0] avg_epoch_loss=6.015518\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=6.015517711639404\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] Epoch[18] Batch[5] avg_epoch_loss=5.973828\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=5.973828236262004\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:02 INFO 139987855828352] Epoch[18] Batch [5]#011Speed: 348.62 samples/sec#011loss=5.973828\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] Epoch[18] Batch[10] avg_epoch_loss=5.954143\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=5.930520534515381\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] Epoch[18] Batch [10]#011Speed: 339.11 samples/sec#011loss=5.930521\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] processed a total of 339 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545742.0765069, \"EndTime\": 1622545743.2672112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1190.202236175537, \"count\": 1, \"min\": 1190.202236175537, \"max\": 1190.202236175537}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=284.79342583362325 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] #quality_metric: host=algo-1, epoch=18, train loss <loss>=5.954142917286266\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] Epoch[19] Batch[0] avg_epoch_loss=5.989480\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=5.989480018615723\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] Epoch[19] Batch[5] avg_epoch_loss=5.933712\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=5.933712164560954\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:03 INFO 139987855828352] Epoch[19] Batch [5]#011Speed: 346.74 samples/sec#011loss=5.933712\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] Epoch[19] Batch[10] avg_epoch_loss=5.948967\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=5.967273616790772\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] Epoch[19] Batch [10]#011Speed: 279.95 samples/sec#011loss=5.967274\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] processed a total of 342 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545743.2673013, \"EndTime\": 1622545744.5277712, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1259.9573135375977, \"count\": 1, \"min\": 1259.9573135375977, \"max\": 1259.9573135375977}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] #throughput_metric: host=algo-1, train throughput=271.4076715677463 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] #quality_metric: host=algo-1, epoch=19, train loss <loss>=5.948967370119962\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] Final loss: 5.931105050173673 (occurred at epoch 16)\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] #quality_metric: host=algo-1, train final_loss <loss>=5.931105050173673\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 WARNING 139987855828352] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:04 INFO 139987855828352] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545744.5278492, \"EndTime\": 1622545745.026822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 497.94721603393555, \"count\": 1, \"min\": 497.94721603393555, \"max\": 497.94721603393555}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:05 INFO 139987855828352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545745.0268993, \"EndTime\": 1622545745.1790538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 650.2199172973633, \"count\": 1, \"min\": 650.2199172973633, \"max\": 650.2199172973633}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:05 INFO 139987855828352] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:05 INFO 139987855828352] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545745.1791146, \"EndTime\": 1622545745.2004657, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 21.308183670043945, \"count\": 1, \"min\": 21.308183670043945, \"max\": 21.308183670043945}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:05 INFO 139987855828352] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:05 INFO 139987855828352] #memory_usage::<batchbuffer> = 3.4353256225585938 mb\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:05 INFO 139987855828352] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545745.20052, \"EndTime\": 1622545745.2019484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.034332275390625, \"count\": 1, \"min\": 0.034332275390625, \"max\": 0.034332275390625}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545745.2020004, \"EndTime\": 1622545746.993041, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 1791.1288738250732, \"count\": 1, \"min\": 1791.1288738250732, \"max\": 1791.1288738250732}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, RMSE): 157.76598175779213\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, mean_absolute_QuantileLoss): 4268.821325005426\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, mean_wQuantileLoss): 0.1350739794179258\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.1]): 0.07098537605726409\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.2]): 0.11359553426977592\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.3]): 0.14378230645149864\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.4]): 0.15926487398181716\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.5]): 0.16950425189508528\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.6]): 0.17124947410940175\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.7]): 0.15678214041653743\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.8]): 0.13565541331825043\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #test_score (algo-1, wQuantileLoss[0.9]): 0.09484644426170134\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #quality_metric: host=algo-1, test RMSE <loss>=157.76598175779213\u001b[0m\n",
      "\u001b[34m[06/01/2021 11:09:06 INFO 139987855828352] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.1350739794179258\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1622545746.9931164, \"EndTime\": 1622545747.0260742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.065057754516602, \"count\": 1, \"min\": 7.065057754516602, \"max\": 7.065057754516602}, \"totaltime\": {\"sum\": 27773.29444885254, \"count\": 1, \"min\": 27773.29444885254, \"max\": 27773.29444885254}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-01 11:09:12 Uploading - Uploading generated training model\n",
      "2021-06-01 11:09:32 Completed - Training job completed\n",
      "ProfilerReport-1622545517: NoIssuesFound\n",
      "Training seconds: 83\n",
      "Billable seconds: 83\n",
      "CPU times: user 600 ms, sys: 43.3 ms, total: 643 ms\n",
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-cookbook-bucket/chapter08/output/forecasting-deepar-2021-06-01-11-05-17-028/output/model.tar.gz'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store endpoint_name"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
