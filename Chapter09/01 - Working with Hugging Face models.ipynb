{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"raw.githubusercontent.com\"\n",
    "p = \"PacktPublishing\"\n",
    "a = \"Amazon-SageMaker-Cookbook\"\n",
    "mc = \"master/Chapter09\"\n",
    "\n",
    "path = f\"https://{g}/{p}/{a}/{mc}/scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 02:29:04--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/scripts/setup.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 181 [text/plain]\n",
      "Saving to: ‘scripts/setup.py.1’\n",
      "\n",
      "setup.py.1          100%[===================>]     181  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-08 02:29:04 (6.00 MB/s) - ‘scripts/setup.py.1’ saved [181/181]\n",
      "\n",
      "--2021-06-08 02:29:04--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/scripts/train.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4840 (4.7K) [text/plain]\n",
      "Saving to: ‘scripts/train.py.1’\n",
      "\n",
      "train.py.1          100%[===================>]   4.73K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-08 02:29:05 (56.7 MB/s) - ‘scripts/train.py.1’ saved [4840/4840]\n",
      "\n",
      "--2021-06-08 02:29:05--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/scripts/inference.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1358 (1.3K) [text/plain]\n",
      "Saving to: ‘scripts/inference.py.1’\n",
      "\n",
      "inference.py.1      100%[===================>]   1.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-08 02:29:05 (42.0 MB/s) - ‘scripts/inference.py.1’ saved [1358/1358]\n",
      "\n",
      "--2021-06-08 02:29:05--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/scripts/requirements.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19 [text/plain]\n",
      "Saving to: ‘scripts/requirements.txt.1’\n",
      "\n",
      "requirements.txt.1  100%[===================>]      19  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-08 02:29:05 (631 KB/s) - ‘scripts/requirements.txt.1’ saved [19/19]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P scripts {path}/setup.py\n",
    "!wget -P scripts {path}/train.py\n",
    "!wget -P scripts {path}/inference.py\n",
    "!wget -P scripts {path}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"raw.githubusercontent.com\"\n",
    "p = \"PacktPublishing\"\n",
    "a = \"Amazon-SageMaker-Cookbook\"\n",
    "mc = \"master/Chapter09\"\n",
    "\n",
    "path = f\"https://{g}/{p}/{a}/{mc}/files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 02:29:06--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/files/synthetic.train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154276 (151K) [text/plain]\n",
      "Saving to: ‘tmp/synthetic.train.txt.1’\n",
      "\n",
      "synthetic.train.txt 100%[===================>] 150.66K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-06-08 02:29:06 (34.1 MB/s) - ‘tmp/synthetic.train.txt.1’ saved [154276/154276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P tmp {path}/synthetic.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 02:29:06--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/files/synthetic.validation.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50653 (49K) [text/plain]\n",
      "Saving to: ‘tmp/synthetic.validation.txt.1’\n",
      "\n",
      "synthetic.validatio 100%[===================>]  49.47K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-06-08 02:29:06 (94.0 MB/s) - ‘tmp/synthetic.validation.txt.1’ saved [50653/50653]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P tmp {path}/synthetic.validation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"sagemaker-cookbook-bucket\"\n",
    "prefix = \"chapter09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/input/{}'.format(\n",
    "    s3_bucket, \n",
    "    prefix, \n",
    "    \"synthetic.train.txt\"\n",
    ")\n",
    "s3_validation_data = 's3://{}/{}/input/{}'.format(\n",
    "    s3_bucket, \n",
    "    prefix, \n",
    "    \"synthetic.validation.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: tmp/synthetic.train.txt to s3://sagemaker-cookbook-bucket/chapter09/input/synthetic.train.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tmp/synthetic.train.txt {s3_train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: tmp/synthetic.validation.txt to s3://sagemaker-cookbook-bucket/chapter09/input/synthetic.validation.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tmp/synthetic.validation.txt {s3_validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.4',\n",
    "    pytorch_version='1.6',\n",
    "    py_version='py36',\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_data = TrainingInput(s3_train_data)\n",
    "validation_data = TrainingInput(s3_validation_data)\n",
    "\n",
    "data_channels = {\n",
    "    'train': train_data, \n",
    "    'valid': validation_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-08 02:29:10 Starting - Starting the training job...\n",
      "2021-06-08 02:29:13 Starting - Launching requested ML instancesProfilerReport-1623119350: InProgress\n",
      "......\n",
      "2021-06-08 02:30:26 Starting - Preparing the instances for training.........\n",
      "2021-06-08 02:32:07 Downloading - Downloading input data...\n",
      "2021-06-08 02:32:27 Training - Downloading the training image.................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:19,571 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:19,594 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:21,012 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:21,329 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.4.2 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (0.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (0.0.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (2021.3.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.4.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: distillbert\n",
      "  Building wheel for distillbert (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for distillbert (setup.py): finished with status 'done'\n",
      "  Created wheel for distillbert: filename=distillbert-1.0-py3-none-any.whl size=1012 sha256=a5d129110a7e5008cea49182d984cd359c2d5882482a1dbcab2935decf14f120\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dd_o3op5/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built distillbert\u001b[0m\n",
      "\u001b[34mInstalling collected packages: distillbert\u001b[0m\n",
      "\u001b[34mSuccessfully installed distillbert-1.0\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:24,419 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"valid\": \"/opt/ml/input/data/valid\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-06-08-02-29-10-202\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-581320662326/huggingface-pytorch-training-2021-06-08-02-29-10-202/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-581320662326/huggingface-pytorch-training-2021-06-08-02-29-10-202/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-06-08-02-29-10-202\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-581320662326/huggingface-pytorch-training-2021-06-08-02-29-10-202/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m train --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-08 02:35:28 Training - Training image download completed. Training in progress.\u001b[34m2021-06-08 02:35:28,512 - \u001b[0m\n",
      "\u001b[34m------- 01 --------\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:28,573 - \u001b[0m\n",
      "\u001b[34m------- 02 --------\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:28,802 - Lock 139910338423776 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:28,823 - Lock 139910338423776 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:28,849 - Lock 139910338424224 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:33,794 - Lock 139910338424224 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:36,122 - \u001b[0m\n",
      "\u001b[34m------- 03 --------\n",
      "\u001b[0m\n",
      "\u001b[34mDONE\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.515 algo-1:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.694 algo-1:37 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.694 algo-1:37 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.695 algo-1:37 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.788 algo-1:37 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.788 algo-1:37 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.946 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.946 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.946 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.946 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.946 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.947 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.948 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.949 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.950 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.951 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.952 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.953 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.954 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.955 algo-1:37 INFO hook.py:550] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.956 algo-1:37 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.956 algo-1:37 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:39.958 algo-1:37 INFO hook.py:476] Hook is writing from the hook with pid: 37\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:40.933 algo-1:37 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:40.934 algo-1:37 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-06-08 02:35:40.954 algo-1:37 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.7033, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m{'loss': 0.7012, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6839, 'learning_rate': 3e-06, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6664, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6556, 'learning_rate': 5e-06, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m{'loss': 0.613, 'learning_rate': 6e-06, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m{'loss': 0.5222, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m{'loss': 0.366, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2236, 'learning_rate': 9e-06, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1125, 'learning_rate': 1e-05, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m{'loss': 0.074, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.47}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0282, 'learning_rate': 1.2e-05, 'epoch': 1.6}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0327, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0524, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0291, 'learning_rate': 1.5e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0077, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0264, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.27}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0136, 'learning_rate': 1.8e-05, 'epoch': 2.4}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0136, 'learning_rate': 1.9e-05, 'epoch': 2.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0038, 'learning_rate': 2e-05, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0153, 'learning_rate': 2.1e-05, 'epoch': 2.8}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0244, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.93}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 14.4005, 'train_samples_per_second': 15.624, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m2021-06-08 02:35:53,933 - \u001b[0m\n",
      "\u001b[34m------- 04 --------\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-08 02:36:10 Uploading - Uploading generated training model\u001b[34m2021-06-08 02:36:07,117 - \u001b[0m\n",
      "\u001b[34m------- 05 --------\n",
      "\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 47.7MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 49.3MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 40.3kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 580kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.59M/268M [00:00<00:05, 45.9MB/s]#015Downloading:   4%|▎         | 9.66M/268M [00:00<00:05, 47.2MB/s]#015Downloading:   6%|▌         | 14.8M/268M [00:00<00:05, 48.4MB/s]#015Downloading:   7%|▋         | 19.9M/268M [00:00<00:05, 49.2MB/s]#015Downloading:   9%|▉         | 25.3M/268M [00:00<00:04, 50.6MB/s]#015Downloading:  12%|█▏        | 30.9M/268M [00:00<00:04, 51.9MB/s]#015Downloading:  14%|█▎        | 36.4M/268M [00:00<00:04, 53.0MB/s]#015Downloading:  16%|█▌        | 42.0M/268M [00:00<00:04, 53.7MB/s]#015Downloading:  18%|█▊        | 47.5M/268M [00:00<00:04, 54.1MB/s]#015Downloading:  20%|█▉        | 52.7M/268M [00:01<00:04, 53.3MB/s]#015Downloading:  22%|██▏       | 58.4M/268M [00:01<00:03, 54.2MB/s]#015Downloading:  24%|██▍       | 64.0M/268M [00:01<00:03, 54.9MB/s]#015Downloading:  26%|██▌       | 69.7M/268M [00:01<00:03, 55.3MB/s]#015Downloading:  28%|██▊       | 75.3M/268M [00:01<00:03, 55.7MB/s]#015Downloading:  30%|███       | 80.9M/268M [00:01<00:03, 55.7MB/s]#015Downloading:  32%|███▏      | 86.6M/268M [00:01<00:03, 56.0MB/s]#015Downloading:  34%|███▍      | 92.3M/268M [00:01<00:03, 56.3MB/s]#015Downloading:  37%|███▋      | 98.0M/268M [00:01<00:03, 56.6MB/s]#015Downloading:  39%|███▊      | 104M/268M [00:01<00:02, 56.7MB/s] #015Downloading:  41%|████      | 110M/268M [00:02<00:02, 57.1MB/s]#015Downloading:  43%|████▎     | 115M/268M [00:02<00:02, 57.3MB/s]#015Downloading:  45%|████▌     | 121M/268M [00:02<00:02, 57.3MB/s]#015Downloading:  47%|████▋     | 127M/268M [00:02<00:02, 57.5MB/s]#015Downloading:  49%|████▉     | 133M/268M [00:02<00:02, 57.7MB/s]#015Downloading:  52%|█████▏    | 138M/268M [00:02<00:02, 57.0MB/s]#015Downloading:  54%|█████▍    | 144M/268M [00:02<00:02, 57.3MB/s]#015Downloading:  56%|█████▌    | 150M/268M [00:02<00:02, 57.5MB/s]#015Downloading:  58%|█████▊    | 156M/268M [00:02<00:01, 57.7MB/s]#015Downloading:  60%|██████    | 162M/268M [00:02<00:01, 58.0MB/s]#015Downloading:  63%|██████▎   | 168M/268M [00:03<00:01, 58.1MB/s]#015Downloading:  65%|██████▍   | 173M/268M [00:03<00:01, 58.2MB/s]#015Downloading:  67%|██████▋   | 179M/268M [00:03<00:01, 58.1MB/s]#015Downloading:  69%|██████▉   | 185M/268M [00:03<00:01, 58.3MB/s]#015Downloading:  71%|███████   | 191M/268M [00:03<00:01, 58.4MB/s]#015Downloading:  73%|███████▎  | 197M/268M [00:03<00:01, 57.6MB/s]#015Downloading:  76%|███████▌  | 203M/268M [00:03<00:01, 57.8MB/s]#015Downloading:  78%|███████▊  | 208M/268M [00:03<00:01, 57.9MB/s]#015Downloading:  80%|███████▉  | 214M/268M [00:03<00:00, 58.0MB/s]#015Downloading:  82%|████████▏ | 220M/268M [00:03<00:00, 58.3MB/s]#015Downloading:  84%|████████▍ | 226M/268M [00:04<00:00, 58.4MB/s]#015Downloading:  87%|████████▋ | 232M/268M [00:04<00:00, 58.6MB/s]#015Downloading:  89%|████████▊ | 238M/268M [00:04<00:00, 58.4MB/s]#015Downloading:  91%|█████████ | 244M/268M [00:04<00:00, 58.3MB/s]#015Downloading:  93%|█████████▎| 249M/268M [00:04<00:00, 58.3MB/s]#015Downloading:  95%|█████████▌| 255M/268M [00:04<00:00, 57.5MB/s]#015Downloading:  97%|█████████▋| 261M/268M [00:04<00:00, 57.7MB/s]#015Downloading: 100%|█████████▉| 267M/268M [00:04<00:00, 57.9MB/s]#015Downloading: 100%|██████████| 268M/268M [00:04<00:00, 56.6MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/225 [00:00<?, ?it/s]#015  0%|          | 1/225 [00:01<05:50,  1.56s/it]#015  1%|▏         | 3/225 [00:01<04:07,  1.12s/it]#015  2%|▏         | 5/225 [00:01<02:55,  1.25it/s]#015  3%|▎         | 7/225 [00:01<02:05,  1.74it/s]#015  4%|▍         | 9/225 [00:02<01:30,  2.38it/s]#015                                               #015#015  4%|▍         | 10/225 [00:02<01:30,  2.38it/s]#015  5%|▍         | 11/225 [00:02<01:06,  3.20it/s]#015  6%|▌         | 13/225 [00:02<00:50,  4.24it/s]#015  7%|▋         | 15/225 [00:02<00:38,  5.48it/s]#015  8%|▊         | 17/225 [00:02<00:30,  6.90it/s]#015  8%|▊         | 19/225 [00:02<00:24,  8.40it/s]#015                                                #015#015  9%|▉         | 20/225 [00:02<00:24,  8.40it/s]#015  9%|▉         | 21/225 [00:02<00:20,  9.94it/s]#015 10%|█         | 23/225 [00:02<00:18, 11.07it/s]#015 11%|█         | 25/225 [00:03<00:16, 11.88it/s]#015 12%|█▏        | 27/225 [00:03<00:15, 12.93it/s]#015 13%|█▎        | 29/225 [00:03<00:14, 13.74it/s]#015                                                #015#015 13%|█▎        | 30/225 [00:03<00:14, 13.74it/s]#015 14%|█▍        | 31/225 [00:03<00:13, 14.53it/s]#015 15%|█▍        | 33/225 [00:03<00:12, 15.32it/s]#015 16%|█▌        | 35/225 [00:03<00:11, 15.90it/s]#015 16%|█▋        | 37/225 [00:03<00:11, 16.27it/s]#015 17%|█▋        | 39/225 [00:03<00:11, 16.62it/s]#015                                                #015#015 18%|█▊        | 40/225 [00:03<00:11, 16.62it/s]#015 18%|█▊        | 41/225 [00:03<00:10, 16.88it/s]#015 19%|█▉        | 43/225 [00:04<00:10, 17.08it/s]#015 20%|██        | 45/225 [00:04<00:10, 17.02it/s]#015 21%|██        | 47/225 [00:04<00:10, 17.13it/s]#015 22%|██▏       | 49/225 [00:04<00:10, 17.25it/s]#015                                                #015#015 22%|██▏       | 50/225 [00:04<00:10, 17.25it/s]#015 23%|██▎       | 51/225 [00:04<00:10, 17.32it/s]#015 24%|██▎       | 53/225 [00:04<00:09, 17.38it/s]#015 24%|██▍       | 55/225 [00:04<00:09, 17.43it/s]#015 25%|██▌       | 57/225 [00:04<00:09, 17.45it/s]#015 26%|██▌       | 59/225 [00:04<00:09, 17.49it/s]#015                                                #015#015 27%|██▋       | 60/225 [00:05<00:09, 17.49it/s]#015 27%|██▋       | 61/225 [00:05<00:09, 17.48it/s]#015 28%|██▊       | 63/225 [00:05<00:09, 17.38it/s]#015 29%|██▉       | 65/225 [00:05<00:09, 17.53it/s]#015 30%|██▉       | 67/225 [00:05<00:08, 17.67it/s]#015 31%|███       | 69/225 [00:05<00:08, 17.76it/s]#015                                                #015#015 31%|███       | 70/225 [00:05<00:08, 17.76it/s]#015 32%|███▏      | 71/225 [00:05<00:08, 17.79it/s]#015 32%|███▏      | 73/225 [00:05<00:08, 17.75it/s]#015 33%|███▎      | 75/225 [00:05<00:08, 17.47it/s]#015 34%|███▍      | 77/225 [00:06<00:08, 17.54it/s]#015 35%|███▌      | 79/225 [00:06<00:08, 17.67it/s]#015                                                #015#015 36%|███▌      | 80/225 [00:06<00:08, 17.67it/s]#015 36%|███▌      | 81/225 [00:06<00:08, 17.57it/s]#015 37%|███▋      | 83/225 [00:06<00:08, 17.38it/s]#015 38%|███▊      | 85/225 [00:06<00:08, 16.17it/s]#015 39%|███▊      | 87/225 [00:06<00:08, 16.06it/s]#015 40%|███▉      | 89/225 [00:06<00:08, 15.90it/s]#015                                                #015#015 40%|████      | 90/225 [00:06<00:08, 15.90it/s]#015 40%|████      | 91/225 [00:06<00:08, 15.84it/s]#015 41%|████▏     | 93/225 [00:07<00:08, 15.39it/s]#015 42%|████▏     | 95/225 [00:07<00:08, 15.12it/s]#015 43%|████▎     | 97/225 [00:07<00:08, 14.81it/s]#015 44%|████▍     | 99/225 [00:07<00:08, 15.05it/s]#015                                                #015#015 44%|████▍     | 100/225 [00:07<00:08, 15.05it/s]#015 45%|████▍     | 101/225 [00:07<00:08, 14.96it/s]#015 46%|████▌     | 103/225 [00:07<00:07, 15.87it/s]#015 47%|████▋     | 105/225 [00:07<00:07, 16.47it/s]#015 48%|████▊     | 107/225 [00:07<00:06, 16.89it/s]#015 48%|████▊     | 109/225 [00:08<00:06, 17.24it/s]#015                                                 #015#015 49%|████▉     | 110/225 [00:08<00:06, 17.24it/s]#015 49%|████▉     | 111/225 [00:08<00:06, 17.60it/s]#015 50%|█████     | 113/225 [00:08<00:06, 17.84it/s]#015 51%|█████     | 115/225 [00:08<00:06, 17.85it/s]#015 52%|█████▏    | 117/225 [00:08<00:06, 17.95it/s]#015 53%|█████▎    | 119/225 [00:08<00:05, 18.02it/s]#015                                                 #015#015 53%|█████▎    | 120/225 [00:08<00:05, 18.02it/s]#015 54%|█████▍    | 121/225 [00:08<00:05, 18.11it/s]#015 55%|█████▍    | 123/225 [00:08<00:05, 18.25it/s]#015 56%|█████▌    | 125/225 [00:08<00:05, 18.34it/s]#015 56%|█████▋    | 127/225 [00:08<00:05, 18.32it/s]#015 57%|█████▋    | 129/225 [00:09<00:05, 18.34it/s]#015                                                 #015#015 58%|█████▊    | 130/225 [00:09<00:05, 18.34it/s]#015 58%|█████▊    | 131/225 [00:09<00:05, 18.29it/s]#015 59%|█████▉    | 133/225 [00:09<00:05, 18.17it/s]#015 60%|██████    | 135/225 [00:09<00:04, 18.27it/s]#015 61%|██████    | 137/225 [00:09<00:04, 18.23it/s]#015 62%|██████▏   | 139/225 [00:09<00:04, 18.14it/s]#015                                                 #015#015 62%|██████▏   | 140/225 [00:09<00:04, 18.14it/s]#015 63%|██████▎   | 141/225 [00:09<00:04, 18.05it/s]#015 64%|██████▎   | 143/225 [00:09<00:04, 18.03it/s]#015 64%|██████▍   | 145/225 [00:09<00:04, 18.03it/s]#015 65%|██████▌   | 147/225 [00:10<00:04, 18.19it/s]#015 66%|██████▌   | 149/225 [00:10<00:04, 18.32it/s]#015                                                 #015#015 67%|██████▋   | 150/225 [00:10<00:04, 18.32it/s]#015 67%|██████▋   | 151/225 [00:10<00:04, 18.05it/s]#015 68%|██████▊   | 153/225 [00:10<00:03, 18.18it/s]#015 69%|██████▉   | 155/225 [00:10<00:03, 18.33it/s]#015 70%|██████▉   | 157/225 [00:10<00:03, 18.42it/s]#015 71%|███████   | 159/225 [00:10<00:03, 18.37it/s]#015                                                 #015#015 71%|███████   | 160/225 [00:10<00:03, 18.37it/s]#015 72%|███████▏  | 161/225 [00:10<00:03, 18.24it/s]#015 72%|███████▏  | 163/225 [00:10<00:03, 17.62it/s]#015 73%|███████▎  | 165/225 [00:11<00:03, 17.52it/s]#015 74%|███████▍  | 167/225 [00:11<00:03, 17.74it/s]#015 75%|███████▌  | 169/225 [00:11<00:03, 17.64it/s]#015                                                 #015#015 76%|███████▌  | 170/225 [00:11<00:03, 17.64it/s]#015 76%|███████▌  | 171/225 [00:11<00:03, 17.55it/s]#015 77%|███████▋  | 173/225 [00:11<00:02, 17.75it/s]#015 78%|███████▊  | 175/225 [00:11<00:02, 17.90it/s]#015 79%|███████▊  | 177/225 [00:11<00:02, 18.10it/s]#015 80%|███████▉  | 179/225 [00:11<00:02, 18.16it/s]#015                                                 #015#015 80%|████████  | 180/225 [00:11<00:02, 18.16it/s]#015 80%|████████  | 181/225 [00:11<00:02, 17.90it/s]#015 81%|████████▏ | 183/225 [00:12<00:02, 17.97it/s]#015 82%|████████▏ | 185/225 [00:12<00:02, 18.12it/s]#015 83%|████████▎ | 187/225 [00:12<00:02, 18.02it/s]#015 84%|████████▍ | 189/225 [00:12<00:02, 17.96it/s]#015                                                 #015#015 84%|████████▍ | 190/225 [00:12<00:01, 17.96it/s]#015 85%|████████▍ | 191/225 [00:12<00:01, 18.10it/s]#015 86%|████████▌ | 193/225 [00:12<00:01, 18.16it/s]#015 87%|████████▋ | 195/225 [00:12<00:01, 18.21it/s]#015 88%|████████▊ | 197/225 [00:12<00:01, 18.19it/s]#015 88%|████████▊ | 199/225 [00:12<00:01, 18.27it/s]#015                                                 #015#015 89%|████████▉ | 200/225 [00:13<00:01, 18.27it/s]#015 89%|████████▉ | 201/225 [00:13<00:01, 18.33it/s]#015 90%|█████████ | 203/225 [00:13<00:01, 18.39it/s]#015 91%|█████████ | 205/225 [00:13<00:01, 18.28it/s]#015 92%|█████████▏| 207/225 [00:13<00:00, 18.13it/s]#015 93%|█████████▎| 209/225 [00:13<00:00, 18.20it/s]#015                                                 #015#015 93%|█████████▎| 210/225 [00:13<00:00, 18.20it/s]#015 94%|█████████▍| 211/225 [00:13<00:00, 18.29it/s]#015 95%|█████████▍| 213/225 [00:13<00:00, 18.35it/s]#015 96%|█████████▌| 215/225 [00:13<00:00, 18.26it/s]#015 96%|█████████▋| 217/225 [00:13<00:00, 18.16it/s]#015 97%|█████████▋| 219/225 [00:14<00:00, 18.14it/s]#015                                                 #015#015 98%|█████████▊| 220/225 [00:14<00:00, 18.14it/s]#015 98%|█████████▊| 221/225 [00:14<00:00, 18.19it/s]#015 99%|█████████▉| 223/225 [00:14<00:00, 18.23it/s]#015100%|██████████| 225/225 [00:14<00:00, 18.10it/s]#015                                                 #015#015100%|██████████| 225/225 [00:14<00:00, 18.10it/s]#015100%|██████████| 225/225 [00:14<00:00, 15.63it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/7 [00:00<?, ?it/s]#015 86%|████████▌ | 6/7 [00:00<00:00, 51.81it/s]#015100%|██████████| 7/7 [00:00<00:00, 49.69it/s]\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-08 02:36:07,754 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-06-08 02:37:08 Completed - Training job completed\n",
      "ProfilerReport-1623119350: NoIssuesFound\n",
      "Training seconds: 297\n",
      "Billable seconds: 297\n",
      "CPU times: user 1e+03 ms, sys: 103 ms, total: 1.1 s\n",
      "Wall time: 8min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model_data = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=model_data, \n",
    "    role=role, \n",
    "    source_dir=\"scripts\",\n",
    "    entry_point='inference.py', \n",
    "    framework_version='1.6.0',\n",
    "    py_version=\"py3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 18.2 s, sys: 3.34 s, total: 21.5 s\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictor = model.deploy(\n",
    "    instance_type='ml.m5.xlarge', \n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    \"text\": \"This tastes bad. I hate this place.\"\n",
    "}\n",
    "\n",
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    \"text\": \"Very delicious. I would recommend this to my friends\"\n",
    "}\n",
    "\n",
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
