{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-07 15:46:02--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/files/synthetic.train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154276 (151K) [text/plain]\n",
      "Saving to: ‘tmp/synthetic.train.txt’\n",
      "\n",
      "synthetic.train.txt 100%[===================>] 150.66K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-06-07 15:46:02 (7.36 MB/s) - ‘tmp/synthetic.train.txt’ saved [154276/154276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P tmp {path}/synthetic.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-07 15:46:09--  https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Chapter09/files/synthetic.validation.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50653 (49K) [text/plain]\n",
      "Saving to: ‘tmp/synthetic.validation.txt’\n",
      "\n",
      "synthetic.validatio 100%[===================>]  49.47K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-07 15:46:09 (101 MB/s) - ‘tmp/synthetic.validation.txt’ saved [50653/50653]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P tmp {path}/synthetic.validation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"sagemaker-cookbook-bucket\"\n",
    "prefix = \"chapter09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/input/{}'.format(\n",
    "    s3_bucket, \n",
    "    prefix, \n",
    "    \"synthetic.train.txt\"\n",
    ")\n",
    "s3_validation_data = 's3://{}/{}/input/{}'.format(\n",
    "    s3_bucket, \n",
    "    prefix, \n",
    "    \"synthetic.validation.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: tmp/synthetic.train.txt to s3://sagemaker-cookbook-bucket/chapter09/input/synthetic.train.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tmp/synthetic.train.txt {s3_train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: tmp/synthetic.validation.txt to s3://sagemaker-cookbook-bucket/chapter09/input/synthetic.validation.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp tmp/synthetic.validation.txt {s3_validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.4',\n",
    "    pytorch_version='1.6',\n",
    "    py_version='py36',\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_data = TrainingInput(s3_train_data)\n",
    "validation_data = TrainingInput(s3_validation_data)\n",
    "\n",
    "data_channels = {\n",
    "    'train': train_data, \n",
    "    'valid': validation_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-07 16:31:45 Starting - Starting the training job...\n",
      "2021-06-07 16:32:09 Starting - Launching requested ML instancesProfilerReport-1623083504: InProgress\n",
      "......\n",
      "2021-06-07 16:33:10 Starting - Preparing the instances for training.........\n",
      "2021-06-07 16:34:33 Downloading - Downloading input data...\n",
      "2021-06-07 16:35:10 Training - Downloading the training image..................\n",
      "2021-06-07 16:38:11 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:07,827 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:07,850 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:08,147 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:08,495 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.4.2 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (2021.3.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (0.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (0.0.43)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.4.2->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.4.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.4.2->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: distillbert\n",
      "  Building wheel for distillbert (setup.py): started\n",
      "  Building wheel for distillbert (setup.py): finished with status 'done'\n",
      "  Created wheel for distillbert: filename=distillbert-1.0-py3-none-any.whl size=1012 sha256=49e333de25b30e8175ad50e84934f7ec575007e0b93d081c54e4ab4de266298b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vp0e9qmk/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built distillbert\u001b[0m\n",
      "\u001b[34mInstalling collected packages: distillbert\u001b[0m\n",
      "\u001b[34mSuccessfully installed distillbert-1.0\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:11,605 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"valid\": \"/opt/ml/input/data/valid\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-06-07-16-31-44-114\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-581320662326/huggingface-pytorch-training-2021-06-07-16-31-44-114/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-581320662326/huggingface-pytorch-training-2021-06-07-16-31-44-114/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-06-07-16-31-44-114\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-581320662326/huggingface-pytorch-training-2021-06-07-16-31-44-114/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m train --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:16,128 - \u001b[0m\n",
      "\u001b[34m------- 01 --------\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:16,189 - \u001b[0m\n",
      "\u001b[34m------- 02 --------\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:16,429 - Lock 140577661819872 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:16,454 - Lock 140577661819872 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:16,480 - Lock 140577661820320 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:21,839 - Lock 140577661820320 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:24,148 - \u001b[0m\n",
      "\u001b[34m------- 03 --------\n",
      "\u001b[0m\n",
      "\u001b[34mDONE\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:27.931 algo-1:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.128 algo-1:37 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.129 algo-1:37 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.129 algo-1:37 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.230 algo-1:37 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.230 algo-1:37 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.404 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.405 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.406 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.407 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.408 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.409 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.410 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.411 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.412 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.413 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.414 algo-1:37 INFO hook.py:550] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.415 algo-1:37 INFO hook.py:550] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.415 algo-1:37 INFO hook.py:550] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.415 algo-1:37 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.415 algo-1:37 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:28.418 algo-1:37 INFO hook.py:476] Hook is writing from the hook with pid: 37\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:29.558 algo-1:37 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:29.559 algo-1:37 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-06-07 16:38:29.585 algo-1:37 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.6879, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6839, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6793, 'learning_rate': 3e-06, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6657, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6389, 'learning_rate': 5e-06, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m{'loss': 0.5836, 'learning_rate': 6e-06, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m{'loss': 0.4863, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3469, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2047, 'learning_rate': 9e-06, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1115, 'learning_rate': 1e-05, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0555, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.47}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0267, 'learning_rate': 1.2e-05, 'epoch': 1.6}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0185, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0524, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0277, 'learning_rate': 1.5e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0075, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.13}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0193, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.27}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0229, 'learning_rate': 1.8e-05, 'epoch': 2.4}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0096, 'learning_rate': 1.9e-05, 'epoch': 2.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0049, 'learning_rate': 2e-05, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0155, 'learning_rate': 2.1e-05, 'epoch': 2.8}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0231, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.93}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 14.5743, 'train_samples_per_second': 15.438, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:42,516 - \u001b[0m\n",
      "\u001b[34m------- 04 --------\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:55,638 - \u001b[0m\n",
      "\u001b[34m------- 05 --------\n",
      "\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 44.5MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 45.9MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 30.3kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 555kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.52M/268M [00:00<00:05, 45.2MB/s]#015Downloading:   3%|▎         | 8.05M/268M [00:00<00:06, 41.6MB/s]#015Downloading:   5%|▍         | 12.7M/268M [00:00<00:05, 42.8MB/s]#015Downloading:   7%|▋         | 18.0M/268M [00:00<00:05, 45.6MB/s]#015Downloading:   9%|▊         | 23.4M/268M [00:00<00:05, 47.7MB/s]#015Downloading:  11%|█         | 28.8M/268M [00:00<00:04, 49.6MB/s]#015Downloading:  13%|█▎        | 34.3M/268M [00:00<00:04, 50.9MB/s]#015Downloading:  15%|█▍        | 39.7M/268M [00:00<00:04, 51.9MB/s]#015Downloading:  17%|█▋        | 45.3M/268M [00:00<00:04, 52.9MB/s]#015Downloading:  19%|█▉        | 50.4M/268M [00:01<00:04, 52.1MB/s]#015Downloading:  21%|██        | 56.1M/268M [00:01<00:03, 53.4MB/s]#015Downloading:  23%|██▎       | 61.8M/268M [00:01<00:03, 54.4MB/s]#015Downloading:  25%|██▌       | 67.5M/268M [00:01<00:03, 55.3MB/s]#015Downloading:  27%|██▋       | 73.2M/268M [00:01<00:03, 55.8MB/s]#015Downloading:  29%|██▉       | 78.8M/268M [00:01<00:03, 55.0MB/s]#015Downloading:  31%|███▏      | 84.3M/268M [00:01<00:03, 53.9MB/s]#015Downloading:  33%|███▎      | 89.7M/268M [00:01<00:03, 52.8MB/s]#015Downloading:  35%|███▌      | 94.9M/268M [00:01<00:03, 52.4MB/s]#015Downloading:  37%|███▋      | 100M/268M [00:01<00:03, 52.1MB/s] #015Downloading:  39%|███▉      | 105M/268M [00:02<00:03, 52.1MB/s]#015Downloading:  41%|████▏     | 111M/268M [00:02<00:03, 51.8MB/s]#015Downloading:  43%|████▎     | 116M/268M [00:02<00:02, 52.2MB/s]#015Downloading:  45%|████▌     | 121M/268M [00:02<00:02, 51.8MB/s]#015Downloading:  47%|████▋     | 126M/268M [00:02<00:02, 51.6MB/s]#015Downloading:  49%|████▉     | 132M/268M [00:02<00:02, 51.6MB/s]#015Downloading:  51%|█████     | 137M/268M [00:02<00:02, 51.3MB/s]#015Downloading:  53%|█████▎    | 142M/268M [00:02<00:02, 50.5MB/s]#015Downloading:  55%|█████▍    | 147M/268M [00:02<00:02, 50.7MB/s]#015Downloading:  57%|█████▋    | 152M/268M [00:02<00:02, 51.2MB/s]#015Downloading:  59%|█████▊    | 157M/268M [00:03<00:02, 51.4MB/s]#015Downloading:  61%|██████    | 163M/268M [00:03<00:02, 51.5MB/s]#015Downloading:  63%|██████▎   | 168M/268M [00:03<00:01, 51.7MB/s]#015Downloading:  65%|██████▍   | 173M/268M [00:03<00:01, 51.4MB/s]#015Downloading:  66%|██████▋   | 178M/268M [00:03<00:01, 51.6MB/s]#015Downloading:  68%|██████▊   | 183M/268M [00:03<00:01, 51.6MB/s]#015Downloading:  70%|███████   | 189M/268M [00:03<00:01, 51.9MB/s]#015Downloading:  72%|███████▏  | 194M/268M [00:03<00:01, 51.8MB/s]#015Downloading:  74%|███████▍  | 199M/268M [00:03<00:01, 51.3MB/s]#015Downloading:  76%|███████▌  | 204M/268M [00:03<00:01, 51.6MB/s]#015Downloading:  78%|███████▊  | 209M/268M [00:04<00:01, 51.8MB/s]#015Downloading:  80%|████████  | 215M/268M [00:04<00:01, 52.1MB/s]#015Downloading:  82%|████████▏ | 220M/268M [00:04<00:00, 52.6MB/s]#015Downloading:  84%|████████▍ | 225M/268M [00:04<00:00, 52.7MB/s]#015Downloading:  86%|████████▌ | 231M/268M [00:04<00:00, 52.7MB/s]#015Downloading:  88%|████████▊ | 236M/268M [00:04<00:00, 52.6MB/s]#015Downloading:  90%|████████▉ | 241M/268M [00:04<00:00, 52.5MB/s]#015Downloading:  92%|█████████▏| 246M/268M [00:04<00:00, 52.7MB/s]#015Downloading:  94%|█████████▍| 252M/268M [00:04<00:00, 51.3MB/s]#015Downloading:  96%|█████████▌| 257M/268M [00:04<00:00, 51.6MB/s]#015Downloading:  98%|█████████▊| 262M/268M [00:05<00:00, 51.9MB/s]#015Downloading: 100%|██████████| 268M/268M [00:05<00:00, 52.1MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/225 [00:00<?, ?it/s]#015  0%|          | 1/225 [00:01<06:40,  1.79s/it]#015  1%|▏         | 3/225 [00:01<04:42,  1.27s/it]#015  2%|▏         | 5/225 [00:02<03:19,  1.10it/s]#015  3%|▎         | 7/225 [00:02<02:22,  1.53it/s]#015  4%|▍         | 9/225 [00:02<01:42,  2.11it/s]#015                                               #015#015  4%|▍         | 10/225 [00:02<01:42,  2.11it/s]#015  5%|▍         | 11/225 [00:02<01:15,  2.83it/s]#015  6%|▌         | 13/225 [00:02<00:57,  3.69it/s]#015  7%|▋         | 15/225 [00:02<00:44,  4.74it/s]#015  8%|▊         | 17/225 [00:02<00:34,  6.02it/s]#015  8%|▊         | 19/225 [00:02<00:27,  7.49it/s]#015                                                #015#015  9%|▉         | 20/225 [00:03<00:27,  7.49it/s]#015  9%|▉         | 21/225 [00:03<00:22,  8.99it/s]#015 10%|█         | 23/225 [00:03<00:19, 10.52it/s]#015 11%|█         | 25/225 [00:03<00:16, 11.97it/s]#015 12%|█▏        | 27/225 [00:03<00:14, 13.22it/s]#015 13%|█▎        | 29/225 [00:03<00:14, 13.90it/s]#015                                                #015#015 13%|█▎        | 30/225 [00:03<00:14, 13.90it/s]#015 14%|█▍        | 31/225 [00:03<00:13, 14.56it/s]#015 15%|█▍        | 33/225 [00:03<00:12, 15.12it/s]#015 16%|█▌        | 35/225 [00:03<00:12, 15.72it/s]#015 16%|█▋        | 37/225 [00:04<00:11, 15.77it/s]#015 17%|█▋        | 39/225 [00:04<00:11, 16.22it/s]#015                                                #015#015 18%|█▊        | 40/225 [00:04<00:11, 16.22it/s]#015 18%|█▊        | 41/225 [00:04<00:11, 16.22it/s]#015 19%|█▉        | 43/225 [00:04<00:11, 16.35it/s]#015 20%|██        | 45/225 [00:04<00:10, 16.38it/s]#015 21%|██        | 47/225 [00:04<00:11, 16.15it/s]#015 22%|██▏       | 49/225 [00:04<00:11, 15.85it/s]#015                                                #015#015 22%|██▏       | 50/225 [00:04<00:11, 15.85it/s]#015 23%|██▎       | 51/225 [00:04<00:10, 16.14it/s]#015 24%|██▎       | 53/225 [00:04<00:10, 16.54it/s]#015 24%|██▍       | 55/225 [00:05<00:10, 16.56it/s]#015 25%|██▌       | 57/225 [00:05<00:09, 16.86it/s]#015 26%|██▌       | 59/225 [00:05<00:09, 17.09it/s]#015                                                #015#015 27%|██▋       | 60/225 [00:05<00:09, 17.09it/s]#015 27%|██▋       | 61/225 [00:05<00:09, 17.19it/s]#015 28%|██▊       | 63/225 [00:05<00:09, 17.35it/s]#015 29%|██▉       | 65/225 [00:05<00:09, 17.43it/s]#015 30%|██▉       | 67/225 [00:05<00:09, 17.39it/s]#015 31%|███       | 69/225 [00:05<00:08, 17.47it/s]#015                                                #015#015 31%|███       | 70/225 [00:05<00:08, 17.47it/s]#015 32%|███▏      | 71/225 [00:06<00:08, 17.52it/s]#015 32%|███▏      | 73/225 [00:06<00:08, 17.64it/s]#015 33%|███▎      | 75/225 [00:06<00:08, 17.68it/s]#015 34%|███▍      | 77/225 [00:06<00:08, 17.67it/s]#015 35%|███▌      | 79/225 [00:06<00:08, 17.76it/s]#015                                                #015#015 36%|███▌      | 80/225 [00:06<00:08, 17.76it/s]#015 36%|███▌      | 81/225 [00:06<00:08, 17.77it/s]#015 37%|███▋      | 83/225 [00:06<00:07, 17.75it/s]#015 38%|███▊      | 85/225 [00:06<00:07, 17.67it/s]#015 39%|███▊      | 87/225 [00:06<00:07, 17.65it/s]#015 40%|███▉      | 89/225 [00:07<00:07, 17.71it/s]#015                                                #015#015 40%|████      | 90/225 [00:07<00:07, 17.71it/s]#015 40%|████      | 91/225 [00:07<00:07, 17.73it/s]#015 41%|████▏     | 93/225 [00:07<00:07, 17.76it/s]#015 42%|████▏     | 95/225 [00:07<00:07, 17.85it/s]#015 43%|████▎     | 97/225 [00:07<00:07, 17.96it/s]#015 44%|████▍     | 99/225 [00:07<00:06, 18.17it/s]#015                                                #015#015 44%|████▍     | 100/225 [00:07<00:06, 18.17it/s]#015 45%|████▍     | 101/225 [00:07<00:06, 18.10it/s]#015 46%|████▌     | 103/225 [00:07<00:06, 18.00it/s]#015 47%|████▋     | 105/225 [00:07<00:06, 17.96it/s]#015 48%|████▊     | 107/225 [00:08<00:06, 18.07it/s]#015 48%|████▊     | 109/225 [00:08<00:06, 18.07it/s]#015                                                 #015#015 49%|████▉     | 110/225 [00:08<00:06, 18.07it/s]#015 49%|████▉     | 111/225 [00:08<00:06, 18.18it/s]#015 50%|█████     | 113/225 [00:08<00:06, 18.24it/s]#015 51%|█████     | 115/225 [00:08<00:06, 18.31it/s]#015 52%|█████▏    | 117/225 [00:08<00:05, 18.33it/s]#015 53%|█████▎    | 119/225 [00:08<00:05, 18.30it/s]#015                                                 #015#015 53%|█████▎    | 120/225 [00:08<00:05, 18.30it/s]#015 54%|█████▍    | 121/225 [00:08<00:05, 18.15it/s]#015 55%|█████▍    | 123/225 [00:08<00:05, 18.22it/s]#015 56%|█████▌    | 125/225 [00:09<00:05, 18.31it/s]#015 56%|█████▋    | 127/225 [00:09<00:05, 18.31it/s]#015 57%|█████▋    | 129/225 [00:09<00:05, 18.17it/s]#015                                                 #015#015 58%|█████▊    | 130/225 [00:09<00:05, 18.17it/s]#015 58%|█████▊    | 131/225 [00:09<00:05, 17.97it/s]#015 59%|█████▉    | 133/225 [00:09<00:05, 17.90it/s]#015 60%|██████    | 135/225 [00:09<00:05, 17.72it/s]#015 61%|██████    | 137/225 [00:09<00:05, 17.60it/s]#015 62%|██████▏   | 139/225 [00:09<00:04, 17.59it/s]#015                                                 #015#015 62%|██████▏   | 140/225 [00:09<00:04, 17.59it/s]#015 63%|██████▎   | 141/225 [00:09<00:04, 17.66it/s]#015 64%|██████▎   | 143/225 [00:10<00:04, 17.79it/s]#015 64%|██████▍   | 145/225 [00:10<00:04, 17.80it/s]#015 65%|██████▌   | 147/225 [00:10<00:04, 17.42it/s]#015 66%|██████▌   | 149/225 [00:10<00:04, 17.67it/s]#015                                                 #015#015 67%|██████▋   | 150/225 [00:10<00:04, 17.67it/s]#015 67%|██████▋   | 151/225 [00:10<00:04, 17.71it/s]#015 68%|██████▊   | 153/225 [00:10<00:04, 17.92it/s]#015 69%|██████▉   | 155/225 [00:10<00:03, 17.87it/s]#015 70%|██████▉   | 157/225 [00:10<00:03, 17.98it/s]#015 71%|███████   | 159/225 [00:10<00:03, 17.90it/s]#015                                                 #015#015 71%|███████   | 160/225 [00:10<00:03, 17.90it/s]#015 72%|███████▏  | 161/225 [00:11<00:03, 18.02it/s]#015 72%|███████▏  | 163/225 [00:11<00:03, 18.00it/s]#015 73%|███████▎  | 165/225 [00:11<00:03, 18.12it/s]#015 74%|███████▍  | 167/225 [00:11<00:03, 18.13it/s]#015 75%|███████▌  | 169/225 [00:11<00:03, 18.14it/s]#015                                                 #015#015 76%|███████▌  | 170/225 [00:11<00:03, 18.14it/s]#015 76%|███████▌  | 171/225 [00:11<00:02, 18.21it/s]#015 77%|███████▋  | 173/225 [00:11<00:02, 18.17it/s]#015 78%|███████▊  | 175/225 [00:11<00:02, 18.20it/s]#015 79%|███████▊  | 177/225 [00:11<00:02, 18.02it/s]#015 80%|███████▉  | 179/225 [00:12<00:02, 18.05it/s]#015                                                 #015#015 80%|████████  | 180/225 [00:12<00:02, 18.05it/s]#015 80%|████████  | 181/225 [00:12<00:02, 18.02it/s]#015 81%|████████▏ | 183/225 [00:12<00:02, 18.15it/s]#015 82%|████████▏ | 185/225 [00:12<00:02, 18.19it/s]#015 83%|████████▎ | 187/225 [00:12<00:02, 18.16it/s]#015 84%|████████▍ | 189/225 [00:12<00:01, 18.07it/s]#015                                                 #015#015 84%|████████▍ | 190/225 [00:12<00:01, 18.07it/s]#015 85%|████████▍ | 191/225 [00:12<00:01, 18.05it/s]#015 86%|████████▌ | 193/225 [00:12<00:01, 18.10it/s]#015 87%|████████▋ | 195/225 [00:12<00:01, 17.92it/s]#015 88%|████████▊ | 197/225 [00:13<00:01, 18.02it/s]#015 88%|████████▊ | 199/225 [00:13<00:01, 18.12it/s]#015                                                 #015#015 89%|████████▉ | 200/225 [00:13<00:01, 18.12it/s]#015 89%|████████▉ | 201/225 [00:13<00:01, 18.11it/s]#015 90%|█████████ | 203/225 [00:13<00:01, 18.13it/s]#015 91%|█████████ | 205/225 [00:13<00:01, 18.02it/s]#015 92%|█████████▏| 207/225 [00:13<00:00, 18.07it/s]#015 93%|█████████▎| 209/225 [00:13<00:00, 18.11it/s]#015                                                 #015#015 93%|█████████▎| 210/225 [00:13<00:00, 18.11it/s]#015 94%|█████████▍| 211/225 [00:13<00:00, 18.13it/s]#015 95%|█████████▍| 213/225 [00:13<00:00, 17.96it/s]#015 96%|█████████▌| 215/225 [00:14<00:00, 17.93it/s]#015 96%|█████████▋| 217/225 [00:14<00:00, 18.05it/s]#015 97%|█████████▋| 219/225 [00:14<00:00, 18.14it/s]#015                                                 #015#015 98%|█████████▊| 220/225 [00:14<00:00, 18.14it/s]#015 98%|█████████▊| 221/225 [00:14<00:00, 18.16it/s]#015 99%|█████████▉| 223/225 [00:14<00:00, 18.08it/s]#015100%|██████████| 225/225 [00:14<00:00, 18.17it/s]#015                                                 #015#015100%|██████████| 225/225 [00:14<00:00, 18.17it/s]#015100%|██████████| 225/225 [00:14<00:00, 15.44it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/7 [00:00<?, ?it/s]#015 86%|████████▌ | 6/7 [00:00<00:00, 52.39it/s]#015100%|██████████| 7/7 [00:00<00:00, 50.29it/s]\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 16:38:56,379 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-06-07 16:39:11 Uploading - Uploading generated training model\n",
      "2021-06-07 16:39:42 Completed - Training job completed\n",
      "Training seconds: 309\n",
      "Billable seconds: 309\n",
      "CPU times: user 1.17 s, sys: 81.5 ms, total: 1.25 s\n",
      "Wall time: 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model_data = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=model_data, \n",
    "    role=role, \n",
    "    source_dir=\"scripts\",\n",
    "    entry_point='inference.py', \n",
    "    framework_version='1.6.0',\n",
    "    py_version=\"py3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 18.6 s, sys: 3.28 s, total: 21.9 s\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictor = model.deploy(\n",
    "    instance_type='ml.m5.xlarge', \n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    \"text\": \"This tastes bad. I hate this place.\"\n",
    "}\n",
    "\n",
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    \"text\": \"Very delicious. I would recommend this to my friends\"\n",
    "}\n",
    "\n",
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
